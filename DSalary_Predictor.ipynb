{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22782e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyautogui\n",
    "import urllib.parse\n",
    "import time\n",
    "import pyperclip\n",
    "import quopri\n",
    "from bs4 import BeautifulSoup\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ee5d9",
   "metadata": {},
   "source": [
    "# 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an automatic process to download the job search pages in Linkedin\n",
    "def auto_download_search(url,file_name):\n",
    "    pyautogui.hotkey('ctrl', '2') # switch Google Chrome tabs\n",
    "    time.sleep(1)\n",
    "    \n",
    "    pyautogui.click(207,55) # click the browser's address bar.\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 'a') # select all content in the address bar.\n",
    "    pyperclip.copy(url) # copy the url\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the url\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    pyautogui.hotkey('enter') # access the url\n",
    "    time.sleep(10)\n",
    "    \n",
    "    pyautogui.click(882,1023)\n",
    "    pyautogui.mouseDown() # scroll down the page to look through all job posts\n",
    "    time.sleep(10)\n",
    "    pyautogui.mouseUp()# release the left mouse button\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 's') # save the page to local path\n",
    "    time.sleep(2)\n",
    "    pyperclip.copy(file_name) # copy the file name\n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the file name\n",
    "    time.sleep(2)\n",
    "    pyautogui.hotkey('enter') # save the page with given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430ed6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a process to look through all the pages of search results\n",
    "def search_page_download():\n",
    "    for i in range(0,1000,25):\n",
    "        url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3733476715&f_SB2=1&f_TPR=r2592000&geoId=103644278&keywords=data%20scientist&location=United%20States&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&spellCorrectionEnabled=true&start={i}\"\n",
    "        file_name = f\"data_science_job_{int(i/25)}.mhtml\"\n",
    "        auto_download_search(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb42a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all pages to collect 1000 job posts related to data science\n",
    "search_page_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f4a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract urls of job posts from all the pages\n",
    "def job_url(file_path):\n",
    "    urls = []\n",
    "    # Open the MHTML file in binary mode and parse it\n",
    "    with open(file_path, 'rb') as file:\n",
    "        msg = BytesParser(policy=policy.default).parse(file)\n",
    "\n",
    "    # Decode the HTML part correctly\n",
    "    html_part = None\n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "        if content_type == 'text/html':\n",
    "            html_part = part.get_payload(decode=True)\n",
    "            break\n",
    "    \n",
    "    # parse the HTML\n",
    "    charset = 'utf-8'\n",
    "    decoded_html = html_part.decode(charset)\n",
    "    soup = BeautifulSoup(decoded_html, 'html.parser')\n",
    "    jobs = soup.find(class_ = \"scaffold-layout__list-container\")\n",
    "    job_info = jobs.find_all(class_ = \"disabled ember-view job-card-container__link job-card-list__title\")\n",
    "    # extract the url for each job post\n",
    "    for i in job_info:\n",
    "        url = i['href']\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43066379",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "job_urls = {}\n",
    "i = 0\n",
    "path = \"D:/Study Abroad/course/DSCI441/project/webpages\"\n",
    "\n",
    "# get the urls of all job posts from the local job pages\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        urls = job_url(file_path)\n",
    "        for url in urls:\n",
    "            job_urls[i] = {}\n",
    "            job_urls[i][\"url\"] = url\n",
    "            job_urls[i][\"status\"] = 0\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "388fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually login the linkedin website\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/home\")\n",
    "\n",
    "# wait enough time to log in manually\n",
    "time.sleep(20)  \n",
    "\n",
    "# save the cookies to a file after login\n",
    "pickle.dump(driver.get_cookies(), open(\"D:/Study Abroad/course/DSCI441/project/cookies.pkl\", \"wb\"))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56ea62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use selenium webdriver to iterate all the job pages\n",
    "# set the options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# load the previously saved cookies\n",
    "cookies = pickle.load(open(\"D:/Study Abroad/course/DSCI441/project/cookies.pkl\", \"rb\"))\n",
    "\n",
    "# iterate all job post pages and download them to local path\n",
    "for i in range(0,1000):\n",
    "    index = i\n",
    "    value = job_urls[index]\n",
    "    \n",
    "    # open a new driver after iterating the job post pages for 20 times in order to save memory\n",
    "    if index % 20 == 0:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(\"https://www.linkedin.com/home\")\n",
    "        time.sleep(4)\n",
    "        \n",
    "        # load the cookies to the driver\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    # get the url for job post\n",
    "    url = value[\"url\"]\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # use the aria-label to locate the button\n",
    "    button_aria_label = \"Click to see more description\"\n",
    "    try:\n",
    "        # Wait up to 10 seconds for the button to be clickable\n",
    "        button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, f\"//button[@aria-label='{button_aria_label}']\")))\n",
    "        # click the button \"Show More\" to show the entire job description\n",
    "        button.click()\n",
    "        \n",
    "        time.sleep(2) # wait up for all information displayed\n",
    "        \n",
    "    except Exception as e:\n",
    "        break # if failed, jump to the next iteration\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 's') # save the page to local path\n",
    "    time.sleep(2)\n",
    "    \n",
    "    file_name = \"data_scientist_job_post_\" + str(index)\n",
    "    pyperclip.copy(file_name) # copy the file name\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the file name\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if index % 20 == 0:\n",
    "        pyautogui.hotkey('tab')  # select the save type\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('down')\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('up')  # save file to mhtml\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('enter') # confirm the file tyep\n",
    "        time.sleep(1)\n",
    "    \n",
    "    pyautogui.hotkey('enter') # save\n",
    "    time.sleep(np.random.randint(1,4)) # wait for random time\n",
    "    \n",
    "    if index % 20 == 19:\n",
    "        driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
