{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22782e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyautogui\n",
    "import urllib.parse\n",
    "import time\n",
    "import pyperclip\n",
    "import quopri\n",
    "from bs4 import BeautifulSoup\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "import requests\n",
    "import json\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0ee5d9",
   "metadata": {},
   "source": [
    "# 1.Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a automatic process to download the Job Search Pages in linkedin\n",
    "def auto_download_search(url,file_name):\n",
    "    pyautogui.hotkey('ctrl', '2') # switch Google Chrome tabs\n",
    "    time.sleep(1)\n",
    "    pyautogui.click(207,55) # click the browser's address bar.\n",
    "    time.sleep(0.5)\n",
    "    pyautogui.hotkey('ctrl', 'a') # select all content.\n",
    "    pyperclip.copy(url) # copy the url\n",
    "    time.sleep(0.5)\n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the url\n",
    "    time.sleep(0.5)\n",
    "    pyautogui.hotkey('enter') # access the url\n",
    "    time.sleep(10)\n",
    "    pyautogui.click(882,1023)\n",
    "    pyautogui.mouseDown() # pressing the left mouse button down to go through all job posts\n",
    "    time.sleep(10)\n",
    "    pyautogui.mouseUp()# Release the left mouse button\n",
    "    pyautogui.hotkey('ctrl', 's') # save the page to local path\n",
    "    time.sleep(2)\n",
    "    pyperclip.copy(file_name) # copy the file name\n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the file name\n",
    "    time.sleep(2)\n",
    "    pyautogui.hotkey('enter') # save the page with given name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "430ed6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a process to jump to the next 25 search records \n",
    "def search_page_download():\n",
    "    for i in range(0,1000,25):\n",
    "        url = f\"https://www.linkedin.com/jobs/search/?currentJobId=3733476715&f_SB2=1&f_TPR=r2592000&geoId=103644278&keywords=data%20scientist&location=United%20States&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&sortBy=R&spellCorrectionEnabled=true&start={i}\"\n",
    "        file_name = f\"data_science_job_{int(i/25)}.mhtml\"\n",
    "        auto_download_search(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb42a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download all pages to collect 1000 job posts related to data science\n",
    "search_page_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0f4a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract urls of jobs from the all the pages\n",
    "def job_url(file_path):\n",
    "    urls = []\n",
    "    # Open the MHTML file in binary mode and parse it\n",
    "    with open(file_path, 'rb') as file:\n",
    "        msg = BytesParser(policy=policy.default).parse(file)\n",
    "\n",
    "    # Decode the HTML part correctly\n",
    "    html_part = None\n",
    "    for part in msg.walk():\n",
    "        content_type = part.get_content_type()\n",
    "        if content_type == 'text/html':\n",
    "            html_part = part.get_payload(decode=True)\n",
    "            break\n",
    "\n",
    "    charset = 'utf-8'\n",
    "    decoded_html = html_part.decode(charset)\n",
    "    soup = BeautifulSoup(decoded_html, 'html.parser')\n",
    "    jobs = soup.find(class_ = \"scaffold-layout__list-container\")\n",
    "    job_info = jobs.find_all(class_ = \"disabled ember-view job-card-container__link job-card-list__title\")\n",
    "    for i in job_info:\n",
    "        url = i['href']\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43066379",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get all urls of job pages in the local path\n",
    "job_urls = {}\n",
    "i = 0\n",
    "path = \"D:/Study Abroad/course/DSCI441/project/webpages\"\n",
    "# walk through all acoustic guitar files\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        urls = job_url(file_path)\n",
    "        for url in urls:\n",
    "            job_urls[i] = {}\n",
    "            job_urls[i][\"url\"] = url\n",
    "            job_urls[i][\"status\"] = 0\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "388fe551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually log in or automate the login process.\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.linkedin.com/home\")\n",
    "\n",
    "# Wait enough time to log in manually\n",
    "time.sleep(20)  \n",
    "\n",
    "# Save the cookies to a file after login\n",
    "pickle.dump(driver.get_cookies(), open(\"D:/Study Abroad/course/DSCI441/project/cookies.pkl\", \"wb\"))\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56ea62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use selenium webdriver to iterate all the job pages\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Load the previously saved cookies\n",
    "cookies = pickle.load(open(\"D:/Study Abroad/course/DSCI441/project/cookies.pkl\", \"rb\"))\n",
    "\n",
    "# iterate all job pages and download them to local\n",
    "for i in range(960,1000):\n",
    "    index = i\n",
    "    value = job_urls[index]\n",
    "    \n",
    "    if index % 20 == 0:\n",
    "        # launch a new driver for every 20 iterates\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(\"https://www.linkedin.com/home\")\n",
    "        time.sleep(4)\n",
    "\n",
    "        for cookie in cookies:\n",
    "            driver.add_cookie(cookie)\n",
    "        \n",
    "        time.sleep(1)\n",
    "    \n",
    "    url = value[\"url\"]\n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Use the aria-label to locate the button\n",
    "    button_aria_label = \"Click to see more description\"\n",
    "    try:\n",
    "        # Wait up to 10 seconds for the button to be clickable\n",
    "        button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, f\"//button[@aria-label='{button_aria_label}']\")))\n",
    "        # Click the button to get all job description\n",
    "        button.click()\n",
    "        \n",
    "        time.sleep(2) # wait up for all information displayed\n",
    "        \n",
    "    except Exception as e:\n",
    "        break\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 's') # save the page to local path\n",
    "    time.sleep(2)\n",
    "    \n",
    "    file_name = \"data_scientist_job_post_\" + str(index)\n",
    "    pyperclip.copy(file_name) # copy the file name\n",
    "    \n",
    "    pyautogui.hotkey('ctrl', 'v') # paste the file name\n",
    "    time.sleep(2)\n",
    "    \n",
    "    if index % 20 == 0:\n",
    "        pyautogui.hotkey('tab')  # select the save type\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('down')\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('up')  # save file to mhtml\n",
    "        time.sleep(1)\n",
    "\n",
    "        pyautogui.hotkey('enter') # confirm the file tyep\n",
    "        time.sleep(1)\n",
    "    \n",
    "    pyautogui.hotkey('enter') # save\n",
    "    time.sleep(np.random.randint(1,4)) # wait for random time\n",
    "    \n",
    "    if index % 20 == 19:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca08e078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/view/3821010167/?eBP=CwEAAAGN4R1MIuqdMOLY_XmmcFxAx5pnn88Wo6XjN3MhCjp34hUJihTrcjU5Yw8328P75RHX7-AcveUp6glvELk8givp8WKtK15OI1n0u_BUejzq9fRwu78aOm_YpUAVjamICJqT1oUqx_vE76fmxwuHcqZF_VFPUYEwUSyZXjx4-VbVF_QAii6TWMqDFICRPyY9P_ECpkXtXix4GYiZVM41fNld9fySE74bDNil7H1zu4ANJ6IDSIio1AHC7vNzTSGiZgvKLAxjy3ZKOfneFuPfTe0J1qhOVRAkk2FpWSnDVIQHvs1V5bQIQZEWYnteigUvSv0NJNS5g-eZXqQV4pa5CAvYiOSTPPrEpfeYI9IZEBgEpFIU27Zdjfd1Yjc68JbLwL34Hy8HRX-Lz3Pv0Ld75gP5xMo&refId=lgXrqZhbD1ni5pxx7G2rLg%3D%3D&trackingId=2Vuqhr4g13v5OHI21lWVoQ%3D%3D&trk=flagship3_search_srp_jobs\n"
     ]
    }
   ],
   "source": [
    "print(job_urls[964]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a35600b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e085d47f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
